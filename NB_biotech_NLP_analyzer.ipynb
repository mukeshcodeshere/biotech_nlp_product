{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is there new filing?\n",
    "#### Compare 10Q vs prior quarter\n",
    "#### Compare 10K vs 10K prior quarter\n",
    "#### portions of 10K similar to 3Q 10Q, compare K vs Q\n",
    "#### https://www.intelligize.com/wp-content/uploads/2023/04/Using-the-Redline-Tool-23.pdf\n",
    "#### https://capedge.com/transcript/1727196/2024Q2/SRRK\n",
    "#### multiple newsdrops in the morning --> 3.05 Pacific time , 4.05 , 5.05, 6.15 AM ; 1.05PM , 1.20PM , 1.45PM , 2.45PM , 4PM --> conference presentation or earnings call \n",
    "\n",
    "1) Check SEC website , conference presentation or earnings call \n",
    "2) Check CapEdge for new transcripts filing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import logging\n",
    "import warnings\n",
    "import json\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "from typing import List, Dict, Optional\n",
    "import datamule as dm\n",
    "import pandas as pd\n",
    "from selectolax.parser import HTMLParser\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Get today's date\n",
    "today = datetime.today()\n",
    "\n",
    "# Format the date in 'YYYY-MM-DD' format\n",
    "today_date = today.strftime('%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading files:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading files: 100%|██████████| 1/1 [00:00<00:00,  1.09it/s]\n",
      "ERROR:__main__:Error downloading company concepts for GOOGL: object NoneType can't be used in 'await' expression\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully downloaded 1 out of 1 URLs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading files: 100%|██████████| 1/1 [00:01<00:00,  1.13s/it]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading files: 100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully downloaded 1 out of 1 URLs\n",
      "\n",
      "Successfully downloaded 1 out of 1 URLs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Fetching URLs: 100%|██████████| 10/10 [00:01<00:00,  8.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://efts.sec.gov/LATEST/search-index?ciks=0000789019&forms=10-K%2C10-Q%2C8-K&startdt=2024-01-01&enddt=2024-06-01\n",
      "Total filings: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading files:   0%|          | 0/6 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading files:  17%|█▋        | 1/6 [00:00<00:02,  2.48it/s]\n",
      "\n",
      "\n",
      "Downloading files:  67%|██████▋   | 4/6 [00:00<00:00,  9.28it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading files: 100%|██████████| 6/6 [00:00<00:00,  7.22it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully downloaded 6 out of 6 URLs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Fetching URLs: 100%|██████████| 10/10 [00:02<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://efts.sec.gov/LATEST/search-index?ciks=0001652044&forms=10-K%2C10-Q%2C8-K&startdt=2024-01-01&enddt=2024-06-01\n",
      "Total filings: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading files:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "Fetching URLs: 100%|██████████| 10/10 [00:02<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://efts.sec.gov/LATEST/search-index?ciks=0000320193&forms=10-K%2C10-Q%2C8-K&startdt=2024-01-01&enddt=2024-06-01\n",
      "Total filings: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading files: 100%|██████████| 5/5 [00:00<00:00, 10.04it/s]\n",
      "ERROR:__main__:Error downloading filings for GOOGL: object NoneType can't be used in 'await' expression\n",
      "ERROR:__main__:Error downloading company concepts for MSFT: object NoneType can't be used in 'await' expression\n",
      "ERROR:__main__:Error downloading filings for MSFT: object NoneType can't be used in 'await' expression\n",
      "ERROR:__main__:Error downloading company concepts for AAPL: object NoneType can't be used in 'await' expression\n",
      "Downloading files: 100%|██████████| 6/6 [00:00<00:00, 14.37it/s]\n",
      "ERROR:__main__:Error downloading filings for AAPL: object NoneType can't be used in 'await' expression\n",
      "INFO:__main__:Download process completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully downloaded 5 out of 5 URLs\n",
      "\n",
      "Successfully downloaded 6 out of 6 URLs\n"
     ]
    }
   ],
   "source": [
    "class SECDownloader:\n",
    "    def __init__(self):\n",
    "        self.downloader = dm.Downloader()\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "    def set_user_agent(self, user_agent: str) -> None:\n",
    "        \"\"\"Set SEC user agent information.\"\"\"\n",
    "        try:\n",
    "            self.downloader.set_headers(user_agent)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to set user agent: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    async def download_filings(self, ticker: str, start_date: str, end_date: str, output_dir: str) -> None:\n",
    "        \"\"\"Download SEC filings with proper error handling.\"\"\"\n",
    "        try:\n",
    "            await self.downloader.download(\n",
    "                ticker=ticker,\n",
    "                form=['10-K', '10-Q', '8-K'],  # Specify forms explicitly\n",
    "                date=(start_date, end_date),\n",
    "                output_dir=output_dir,\n",
    "                return_urls=False  # Ensure we're downloading files\n",
    "            )\n",
    "        except ValueError as e:\n",
    "            self.logger.error(f\"Value error downloading filings for {ticker}: {str(e)}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error downloading filings for {ticker}: {str(e)}\")\n",
    "\n",
    "    async def download_concepts(self, ticker: str, output_dir: str) -> None:\n",
    "        \"\"\"Download company concepts data with proper error handling.\"\"\"\n",
    "        try:\n",
    "            await self.downloader.download_company_concepts(\n",
    "                ticker=ticker,\n",
    "                output_dir=output_dir\n",
    "            )\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error downloading company concepts for {ticker}: {str(e)}\")\n",
    "\n",
    "    async def process_ticker(self, ticker: str, start: str, end: str, base_dir: str) -> None:\n",
    "        \"\"\"Process a single ticker's downloads.\"\"\"\n",
    "        try:\n",
    "            # Create directory structure\n",
    "            ticker_dir = os.path.join(base_dir, ticker)\n",
    "            filings_dir = os.path.join(ticker_dir, 'filings')\n",
    "            concepts_dir = os.path.join(ticker_dir, 'company_concepts')\n",
    "            \n",
    "            os.makedirs(filings_dir, exist_ok=True)\n",
    "            os.makedirs(concepts_dir, exist_ok=True)\n",
    "\n",
    "            # Download both filings and concepts concurrently\n",
    "            await asyncio.gather(\n",
    "                self.download_filings(ticker, start, end, filings_dir),\n",
    "                self.download_concepts(ticker, concepts_dir)\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to process ticker {ticker}: {str(e)}\")\n",
    "\n",
    "    async def download_all_data(self, tickers: List[str], start: str, end: str, base_dir: str = 'sec_data') -> None:\n",
    "        \"\"\"Download all SEC data for given tickers.\"\"\"\n",
    "        tasks = []\n",
    "        for ticker in tickers:\n",
    "            task = self.process_ticker(ticker, start, end, base_dir)\n",
    "            tasks.append(task)\n",
    "        \n",
    "        await asyncio.gather(*tasks)\n",
    "\n",
    "# Initialize parameters\n",
    "tickers = ['AAPL', 'MSFT', 'GOOGL']\n",
    "start = '2024-01-01'\n",
    "end = '2024-06-01'#today_date\n",
    "base_dir = 'sec_data'\n",
    "\n",
    "# Initialize downloader\n",
    "sec_downloader = SECDownloader()\n",
    "\n",
    "try:\n",
    "    # Set user agent (required by SEC)\n",
    "    sec_downloader.set_user_agent(\"Your Name your@email.com\")\n",
    "    \n",
    "    # Create and run async download task\n",
    "    async def run_downloads():\n",
    "        await sec_downloader.download_all_data(tickers, start, end, base_dir)\n",
    "        \n",
    "    asyncio.run(run_downloads())\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    sec_downloader.logger.warning(\"\\nDownload interrupted by user\")\n",
    "except Exception as e:\n",
    "    sec_downloader.logger.error(f\"Fatal error: {str(e)}\")\n",
    "finally:\n",
    "    sec_downloader.logger.info(\"Download process completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading filings and company concepts for AAPL...\n",
      "ERROR:__main__:Error loading company concepts for AAPL: No company concepts found for AAPL\n",
      "INFO:__main__:Loaded 6 filings for AAPL\n",
      "INFO:__main__:Loading filings and company concepts for MSFT...\n",
      "ERROR:__main__:Error loading company concepts for MSFT: No company concepts found for MSFT\n",
      "INFO:__main__:Loaded 6 filings for MSFT\n",
      "INFO:__main__:Loading filings and company concepts for GOOGL...\n",
      "ERROR:__main__:Error loading company concepts for GOOGL: No company concepts found for GOOGL\n",
      "INFO:__main__:Loaded 5 filings for GOOGL\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>filing_date</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2024-05-03</td>\n",
       "      <td>false 2024 Q2 0000320193 --09-28 P1Y P1Y P1Y P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2024-05-03</td>\n",
       "      <td>true true true true true true true true false ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2024-05-02</td>\n",
       "      <td>false 0000320193 0000320193 2024-05-02 2024-05...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2024-02-28</td>\n",
       "      <td>true true true true true true true true NASDAQ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2024-02-02</td>\n",
       "      <td>false 2024 Q1 0000320193 --09-28 P1Y P1Y P1Y h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2024-02-01</td>\n",
       "      <td>false 0000320193 0000320193 2024-02-01 2024-02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>2024-04-25</td>\n",
       "      <td>0000789019 false 0000789019 2024-04-25 2024-04...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>2024-04-25</td>\n",
       "      <td>0000789019 Q3 --06-30 false http://fasb.org/us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>2024-03-08</td>\n",
       "      <td>0000789019 0000789019 2024-01-17 2024-01-17 00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>2024-01-30</td>\n",
       "      <td>false 0000789019 0000789019 msft:NotesThreePoi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>2024-01-30</td>\n",
       "      <td>--06-30 0000789019 Q2 false 2024 http://fasb.o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>2024-01-19</td>\n",
       "      <td>false 0000789019 0000789019 2024-01-17 2024-01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>GOOGL</td>\n",
       "      <td>2024-04-26</td>\n",
       "      <td>FALSE 2024 Q1 0001652044 --12-31 50 50 http://...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>GOOGL</td>\n",
       "      <td>2024-04-25</td>\n",
       "      <td>FALSE 0001652044 0001652044 2024-04-25 2024-04...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>GOOGL</td>\n",
       "      <td>2024-02-08</td>\n",
       "      <td>FALSE 0001652044 0001652044 2024-02-08 2024-02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>GOOGL</td>\n",
       "      <td>2024-01-31</td>\n",
       "      <td>FALSE 2023 FY 0001652044 P7Y 50 50 http://fasb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>GOOGL</td>\n",
       "      <td>2024-01-30</td>\n",
       "      <td>FALSE 0001652044 0001652044 2024-01-30 2024-01...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ticker filing_date                                            content\n",
       "0    AAPL  2024-05-03  false 2024 Q2 0000320193 --09-28 P1Y P1Y P1Y P...\n",
       "1    AAPL  2024-05-03  true true true true true true true true false ...\n",
       "2    AAPL  2024-05-02  false 0000320193 0000320193 2024-05-02 2024-05...\n",
       "3    AAPL  2024-02-28  true true true true true true true true NASDAQ...\n",
       "4    AAPL  2024-02-02  false 2024 Q1 0000320193 --09-28 P1Y P1Y P1Y h...\n",
       "5    AAPL  2024-02-01  false 0000320193 0000320193 2024-02-01 2024-02...\n",
       "6    MSFT  2024-04-25  0000789019 false 0000789019 2024-04-25 2024-04...\n",
       "7    MSFT  2024-04-25  0000789019 Q3 --06-30 false http://fasb.org/us...\n",
       "8    MSFT  2024-03-08  0000789019 0000789019 2024-01-17 2024-01-17 00...\n",
       "9    MSFT  2024-01-30  false 0000789019 0000789019 msft:NotesThreePoi...\n",
       "10   MSFT  2024-01-30  --06-30 0000789019 Q2 false 2024 http://fasb.o...\n",
       "11   MSFT  2024-01-19  false 0000789019 0000789019 2024-01-17 2024-01...\n",
       "12  GOOGL  2024-04-26  FALSE 2024 Q1 0001652044 --12-31 50 50 http://...\n",
       "13  GOOGL  2024-04-25  FALSE 0001652044 0001652044 2024-04-25 2024-04...\n",
       "14  GOOGL  2024-02-08  FALSE 0001652044 0001652044 2024-02-08 2024-02...\n",
       "15  GOOGL  2024-01-31  FALSE 2023 FY 0001652044 P7Y 50 50 http://fasb...\n",
       "16  GOOGL  2024-01-30  FALSE 0001652044 0001652044 2024-01-30 2024-01..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SECFilingLoader:\n",
    "    def __init__(self, base_dir: str = 'sec_data', concepts_dir: str = 'company_concepts'):\n",
    "        self.base_dir = base_dir\n",
    "        self.concepts_dir = concepts_dir  # Directory where Company Concepts are stored\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def extract_text_from_html(self, file_path: str) -> str:\n",
    "        \"\"\"Extract clean text from HTML filing using selectolax (faster than BeautifulSoup).\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                html_content = f.read()\n",
    "            \n",
    "            # Parse HTML using selectolax\n",
    "            tree = HTMLParser(html_content)\n",
    "            \n",
    "            # Remove script and style elements\n",
    "            for tag in tree.css('script'):\n",
    "                tag.decompose()\n",
    "            for tag in tree.css('style'):\n",
    "                tag.decompose()\n",
    "            \n",
    "            # Extract text with newlines between elements\n",
    "            if tree.body:\n",
    "                text = tree.body.text(separator='\\n')\n",
    "                # Clean up extra whitespace\n",
    "                text = ' '.join(text.split())\n",
    "                return text\n",
    "            return \"\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error extracting text from {file_path}: {str(e)}\")\n",
    "            return \"\"\n",
    "\n",
    "    def classify_filing_type(self, content: str, filename: str) -> str:\n",
    "        \"\"\"Classify the filing type as 10-Q or 10-K based on the content or filename.\"\"\"\n",
    "        if '10-K' in content or '10-K' in filename:\n",
    "            return '10-K'\n",
    "        elif '10-Q' in content or '10-Q' in filename:\n",
    "            return '10-Q'\n",
    "        else:\n",
    "            return 'Other'  # You could also use 'Unknown' if you prefer\n",
    "\n",
    "    def load_filings(self, ticker: str) -> pd.DataFrame:\n",
    "        \"\"\"Load SEC filings data with full text content for a specific ticker.\"\"\"\n",
    "        try:\n",
    "            filings_path = os.path.join(self.base_dir, ticker, 'filings')\n",
    "            filing_files = glob(os.path.join(filings_path, '*.htm'))\n",
    "            \n",
    "            if not filing_files:\n",
    "                raise FileNotFoundError(f\"No filings found for {ticker}\")\n",
    "            \n",
    "            filings_data = []\n",
    "            for file_path in filing_files:\n",
    "                try:\n",
    "                    filename = os.path.basename(file_path)\n",
    "                    accession_num, filing_date = filename.replace('.htm', '').split('_')\n",
    "                    \n",
    "                    # Read and clean the file content\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        content = f.read()\n",
    "                        \n",
    "                    # Parse HTML and extract clean text\n",
    "                    tree = HTMLParser(content)\n",
    "                    \n",
    "                    # Remove script and style elements\n",
    "                    for tag in tree.css('script'):\n",
    "                        tag.decompose()\n",
    "                    for tag in tree.css('style'):\n",
    "                        tag.decompose()\n",
    "                    \n",
    "                    # Extract clean text\n",
    "                    if tree.body:\n",
    "                        clean_text = tree.body.text(separator='\\n')\n",
    "                        # Clean up extra whitespace\n",
    "                        clean_text = ' '.join(clean_text.split())\n",
    "                    else:\n",
    "                        clean_text = \"\"\n",
    "                    \n",
    "                    # Classify filing type\n",
    "                    #filing_type = self.classify_filing_type(clean_text, filename)\n",
    "                    \n",
    "                    filing_info = {\n",
    "                        'ticker': ticker,\n",
    "                        'accession_number': accession_num,\n",
    "                        'filing_date': filing_date,\n",
    "                        'file_path': file_path,\n",
    "                        'file_size': os.path.getsize(file_path),\n",
    "                        'content': clean_text,\n",
    "                        'content_length': len(clean_text),\n",
    "                        #'filing_type': filing_type  # Add filing type column\n",
    "                    }\n",
    "                    filings_data.append(filing_info)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Error processing file {file_path}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            df = pd.DataFrame(filings_data)\n",
    "            \n",
    "            # Convert dates and format columns\n",
    "            df['filing_date'] = pd.to_datetime(df['filing_date'])\n",
    "            df['file_size'] = df['file_size'] / 1024  # Convert to KB\n",
    "            \n",
    "            # Sort by filing date\n",
    "            df = df.sort_values('filing_date', ascending=False)\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading filings for {ticker}: {str(e)}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def load_company_concepts(self, ticker: str) -> pd.DataFrame:\n",
    "        \"\"\"Load Company Concepts for a specific ticker.\"\"\"\n",
    "        try:\n",
    "            concepts_file_path = os.path.join(self.concepts_dir, f'{ticker}_concepts.csv')\n",
    "            \n",
    "            if not os.path.exists(concepts_file_path):\n",
    "                raise FileNotFoundError(f\"No company concepts found for {ticker}\")\n",
    "            \n",
    "            df = pd.read_csv(concepts_file_path)\n",
    "            df['ticker'] = ticker  # Add ticker column for reference\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading company concepts for {ticker}: {str(e)}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def load_all_filings(self, tickers: Optional[List[str]] = None) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"Load filings and company concepts data with text for multiple tickers.\"\"\"\n",
    "        all_data = []  # List to hold the DataFrames for all tickers\n",
    "        if tickers is None:\n",
    "            # Get all tickers from the base directory\n",
    "            tickers = [d for d in os.listdir(self.base_dir) \n",
    "                      if os.path.isdir(os.path.join(self.base_dir, d))]\n",
    "        \n",
    "        for ticker in tickers:\n",
    "            self.logger.info(f\"Loading filings and company concepts for {ticker}...\")\n",
    "            \n",
    "            # Load SEC filings\n",
    "            filings_df = self.load_filings(ticker)\n",
    "            \n",
    "            # Load Company Concepts\n",
    "            concepts_df = self.load_company_concepts(ticker)\n",
    "            \n",
    "            # Merge both dataframes if they are not empty\n",
    "            if not filings_df.empty and not concepts_df.empty:\n",
    "                merged_df = pd.merge(filings_df, concepts_df, on='ticker', how='left')\n",
    "                all_data.append(merged_df)\n",
    "                self.logger.info(f\"Loaded {len(filings_df)} filings and concepts for {ticker}\")\n",
    "            elif not filings_df.empty:\n",
    "                all_data.append(filings_df)\n",
    "                self.logger.info(f\"Loaded {len(filings_df)} filings for {ticker}\")\n",
    "            elif not concepts_df.empty:\n",
    "                all_data.append(concepts_df)\n",
    "                self.logger.info(f\"Loaded company concepts for {ticker}\")\n",
    "        \n",
    "        # Concatenate all dataframes into a single DataFrame\n",
    "        all_data_df = pd.concat(all_data, ignore_index=True)\n",
    "        \n",
    "        # Return the merged data for all tickers as a single DataFrame\n",
    "        return all_data_df\n",
    "\n",
    "# Example usage\n",
    "loader = SECFilingLoader('sec_data', 'company_concepts')\n",
    "\n",
    "# Load data for specific tickers\n",
    "tickers = ['AAPL', 'MSFT', 'GOOGL']\n",
    "\n",
    "# Load all filings and company concepts and get one large DataFrame\n",
    "all_data_df = loader.load_all_filings(tickers)\n",
    "\n",
    "required_cols = [\"ticker\",\"filing_date\",\"content\"]#[\"ticker\",\"filing_date\",\"filing_type\",\"content\"]\n",
    "all_data_df_min = all_data_df[required_cols]\n",
    "all_data_df_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import os\n",
    "# # import logging\n",
    "# # import pandas as pd\n",
    "# # from glob import glob\n",
    "# # from selectolax.parser import HTMLParser\n",
    "# # from typing import Optional, List, Dict\n",
    "\n",
    "# # class SECFilingLoader:\n",
    "# #     def __init__(self, base_dir: str = 'sec_data', concepts_dir: str = 'company_concepts'):\n",
    "# #         self.base_dir = base_dir\n",
    "# #         self.concepts_dir = concepts_dir  # Directory where Company Concepts are stored\n",
    "# #         logging.basicConfig(level=logging.INFO)\n",
    "# #         self.logger = logging.getLogger(__name__)\n",
    "\n",
    "# #     def extract_text_from_html(self, file_path: str) -> str:\n",
    "# #         \"\"\"Extract clean text from HTML filing using selectolax (faster than BeautifulSoup).\"\"\"\n",
    "# #         try:\n",
    "# #             with open(file_path, 'r', encoding='utf-8') as f:\n",
    "# #                 html_content = f.read()\n",
    "            \n",
    "# #             # Parse HTML using selectolax\n",
    "# #             tree = HTMLParser(html_content)\n",
    "            \n",
    "# #             # Remove script and style elements\n",
    "# #             for tag in tree.css('script'):\n",
    "# #                 tag.decompose()\n",
    "# #             for tag in tree.css('style'):\n",
    "# #                 tag.decompose()\n",
    "            \n",
    "# #             # Extract text with newlines between elements\n",
    "# #             if tree.body:\n",
    "# #                 text = tree.body.text(separator='\\n')\n",
    "# #                 # Clean up extra whitespace\n",
    "# #                 text = ' '.join(text.split())\n",
    "# #                 return text\n",
    "# #             return \"\"\n",
    "            \n",
    "# #         except Exception as e:\n",
    "# #             self.logger.error(f\"Error extracting text from {file_path}: {str(e)}\")\n",
    "# #             return \"\"\n",
    "\n",
    "# #     def classify_filing_type(self, content: str, filename: str) -> str:\n",
    "# #         \"\"\"Classify the filing type as 10-Q or 10-K based on the content or filename.\"\"\"\n",
    "# #         if '10-K' in content or '10-K' in filename:\n",
    "# #             return '10-K'\n",
    "# #         elif '10-Q' in content or '10-Q' in filename:\n",
    "# #             return '10-Q'\n",
    "# #         else:\n",
    "# #             return 'Other'  # You could also use 'Unknown' if you prefer\n",
    "\n",
    "# #     def load_filings(self, ticker: str) -> pd.DataFrame:\n",
    "# #         \"\"\"Load SEC filings data with full text content for a specific ticker.\"\"\"\n",
    "# #         try:\n",
    "# #             filings_path = os.path.join(self.base_dir, ticker, 'filings')\n",
    "# #             filing_files = glob(os.path.join(filings_path, '*.htm'))\n",
    "            \n",
    "# #             if not filing_files:\n",
    "# #                 raise FileNotFoundError(f\"No filings found for {ticker}\")\n",
    "            \n",
    "# #             filings_data = []\n",
    "# #             for file_path in filing_files:\n",
    "# #                 try:\n",
    "# #                     filename = os.path.basename(file_path)\n",
    "# #                     accession_num, filing_date = filename.replace('.htm', '').split('_')\n",
    "                    \n",
    "# #                     # Read and clean the file content\n",
    "# #                     with open(file_path, 'r', encoding='utf-8') as f:\n",
    "# #                         content = f.read()\n",
    "                        \n",
    "# #                     # Parse HTML and extract clean text\n",
    "# #                     tree = HTMLParser(content)\n",
    "                    \n",
    "# #                     # Remove script and style elements\n",
    "# #                     for tag in tree.css('script'):\n",
    "# #                         tag.decompose()\n",
    "# #                     for tag in tree.css('style'):\n",
    "# #                         tag.decompose()\n",
    "                    \n",
    "# #                     # Extract clean text\n",
    "# #                     if tree.body:\n",
    "# #                         clean_text = tree.body.text(separator='\\n')\n",
    "# #                         # Clean up extra whitespace\n",
    "# #                         clean_text = ' '.join(clean_text.split())\n",
    "# #                     else:\n",
    "# #                         clean_text = \"\"\n",
    "                    \n",
    "# #                     # Classify filing type\n",
    "# #                     filing_type = self.classify_filing_type(clean_text, filename)\n",
    "                    \n",
    "# #                     filing_info = {\n",
    "# #                         'ticker': ticker,\n",
    "# #                         'accession_number': accession_num,\n",
    "# #                         'filing_date': filing_date,\n",
    "# #                         'file_path': file_path,\n",
    "# #                         'file_size': os.path.getsize(file_path),\n",
    "# #                         'content': clean_text,\n",
    "# #                         'content_length': len(clean_text),\n",
    "# #                         'filing_type': filing_type  # Add filing type column\n",
    "# #                     }\n",
    "# #                     filings_data.append(filing_info)\n",
    "                    \n",
    "# #                 except Exception as e:\n",
    "# #                     self.logger.error(f\"Error processing file {file_path}: {str(e)}\")\n",
    "# #                     continue\n",
    "            \n",
    "# #             # Convert to DataFrame\n",
    "# #             df = pd.DataFrame(filings_data)\n",
    "            \n",
    "# #             # Convert dates and format columns\n",
    "# #             df['filing_date'] = pd.to_datetime(df['filing_date'])\n",
    "# #             df['file_size'] = df['file_size'] / 1024  # Convert to KB\n",
    "            \n",
    "# #             # Sort by filing date\n",
    "# #             df = df.sort_values('filing_date', ascending=False)\n",
    "            \n",
    "# #             return df\n",
    "            \n",
    "# #         except Exception as e:\n",
    "# #             self.logger.error(f\"Error loading filings for {ticker}: {str(e)}\")\n",
    "# #             return pd.DataFrame()\n",
    "\n",
    "# #     def load_company_concepts(self, ticker: str) -> pd.DataFrame:\n",
    "# #         \"\"\"Load Company Concepts for a specific ticker.\"\"\"\n",
    "# #         try:\n",
    "# #             concepts_file_path = os.path.join(self.concepts_dir, f'{ticker}_concepts.csv')\n",
    "            \n",
    "# #             if not os.path.exists(concepts_file_path):\n",
    "# #                 raise FileNotFoundError(f\"No company concepts found for {ticker}\")\n",
    "            \n",
    "# #             df = pd.read_csv(concepts_file_path)\n",
    "# #             df['ticker'] = ticker  # Add ticker column for reference\n",
    "# #             return df\n",
    "            \n",
    "# #         except Exception as e:\n",
    "# #             self.logger.error(f\"Error loading company concepts for {ticker}: {str(e)}\")\n",
    "# #             return pd.DataFrame()\n",
    "\n",
    "# #     def load_all_filings(self, tickers: Optional[List[str]] = None) -> Dict[str, pd.DataFrame]:\n",
    "# #         \"\"\"Load filings and company concepts data with text for multiple tickers.\"\"\"\n",
    "# #         all_data = []  # List to hold the DataFrames for all tickers\n",
    "# #         if tickers is None:\n",
    "# #             # Get all tickers from the base directory\n",
    "# #             tickers = [d for d in os.listdir(self.base_dir) \n",
    "# #                       if os.path.isdir(os.path.join(self.base_dir, d))]\n",
    "        \n",
    "# #         for ticker in tickers:\n",
    "# #             self.logger.info(f\"Loading filings and company concepts for {ticker}...\")\n",
    "            \n",
    "# #             # Load SEC filings\n",
    "# #             filings_df = self.load_filings(ticker)\n",
    "            \n",
    "# #             # Load Company Concepts\n",
    "# #             concepts_df = self.load_company_concepts(ticker)\n",
    "            \n",
    "# #             # Merge both dataframes if they are not empty\n",
    "# #             if not filings_df.empty and not concepts_df.empty:\n",
    "# #                 merged_df = pd.merge(filings_df, concepts_df, on='ticker', how='left')\n",
    "# #                 all_data.append(merged_df)\n",
    "# #                 self.logger.info(f\"Loaded {len(filings_df)} filings and concepts for {ticker}\")\n",
    "# #             elif not filings_df.empty:\n",
    "# #                 all_data.append(filings_df)\n",
    "# #                 self.logger.info(f\"Loaded {len(filings_df)} filings for {ticker}\")\n",
    "# #             elif not concepts_df.empty:\n",
    "# #                 all_data.append(concepts_df)\n",
    "# #                 self.logger.info(f\"Loaded company concepts for {ticker}\")\n",
    "        \n",
    "# #         # Concatenate all dataframes into a single DataFrame\n",
    "# #         all_data_df = pd.concat(all_data, ignore_index=True)\n",
    "        \n",
    "# #         # Return the merged data for all tickers as a single DataFrame\n",
    "# #         return all_data_df\n",
    "\n",
    "# # # Example usage\n",
    "# # loader = SECFilingLoader('sec_data', 'company_concepts')\n",
    "\n",
    "# # # Load data for specific tickers\n",
    "# # tickers = ['AAPL', 'MSFT', 'GOOGL']\n",
    "\n",
    "# # # Load all filings and company concepts and get one large DataFrame\n",
    "# # all_data_df = loader.load_all_filings(tickers)\n",
    "\n",
    "# # # Example analysis\n",
    "# # print(f\"\\nTotal rows in all_data_df: {len(all_data_df)}\")\n",
    "# # print(f\"Average text length: {all_data_df['content_length'].mean():.0f} characters\")  # Use 'content_length' here\n",
    "# # print(f\"Date range: {all_data_df['filing_date'].min()} to {all_data_df['filing_date'].max()}\")\n",
    "\n",
    "# # # Optionally, preview the first few rows\n",
    "# # print(\"\\nFirst few rows of all_data_df:\")\n",
    "# # print(all_data_df[['ticker', 'accession_number', 'filing_date', 'filing_type', 'content_length']].head())\n",
    "\n",
    "# ############################################\n",
    "# import os\n",
    "# import logging\n",
    "# import pandas as pd\n",
    "# import json\n",
    "# from glob import glob\n",
    "# from selectolax.parser import HTMLParser\n",
    "# from typing import Optional, List, Dict\n",
    "\n",
    "# class SECFilingLoader:\n",
    "#     def __init__(self, base_dir: str = 'sec_data', concepts_dir: str = 'company_concepts'):\n",
    "#         self.base_dir = base_dir\n",
    "#         self.concepts_dir = concepts_dir  # Directory where Company Concepts are stored\n",
    "#         logging.basicConfig(level=logging.INFO)\n",
    "#         self.logger = logging.getLogger(__name__)\n",
    "\n",
    "#     def extract_text_from_html(self, file_path: str) -> str:\n",
    "#         \"\"\"Extract clean text from HTML filing using selectolax (faster than BeautifulSoup).\"\"\"\n",
    "#         try:\n",
    "#             with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#                 html_content = f.read()\n",
    "            \n",
    "#             # Parse HTML using selectolax\n",
    "#             tree = HTMLParser(html_content)\n",
    "            \n",
    "#             # Remove script and style elements\n",
    "#             for tag in tree.css('script'):\n",
    "#                 tag.decompose()\n",
    "#             for tag in tree.css('style'):\n",
    "#                 tag.decompose()\n",
    "            \n",
    "#             # Extract text with newlines between elements\n",
    "#             if tree.body:\n",
    "#                 text = tree.body.text(separator='\\n')\n",
    "#                 # Clean up extra whitespace\n",
    "#                 text = ' '.join(text.split())\n",
    "#                 return text\n",
    "#             return \"\"\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             self.logger.error(f\"Error extracting text from {file_path}: {str(e)}\")\n",
    "#             return \"\"\n",
    "\n",
    "#     def classify_filing_type(self, content: str, filename: str) -> str:\n",
    "#         \"\"\"Classify the filing type as 10-Q or 10-K based on the content or filename.\"\"\"\n",
    "#         if '10-K' in content or '10-K' in filename:\n",
    "#             return '10-K'\n",
    "#         elif '10-Q' in content or '10-Q' in filename:\n",
    "#             return '10-Q'\n",
    "#         else:\n",
    "#             return 'Other'  # You could also use 'Unknown' if you prefer\n",
    "\n",
    "#     def load_filings(self, ticker: str) -> pd.DataFrame:\n",
    "#         \"\"\"Load SEC filings data with full text content for a specific ticker.\"\"\"\n",
    "#         try:\n",
    "#             filings_path = os.path.join(self.base_dir, ticker, 'filings')\n",
    "#             filing_files = glob(os.path.join(filings_path, '*.htm'))\n",
    "            \n",
    "#             if not filing_files:\n",
    "#                 raise FileNotFoundError(f\"No filings found for {ticker}\")\n",
    "            \n",
    "#             filings_data = []\n",
    "#             for file_path in filing_files:\n",
    "#                 try:\n",
    "#                     filename = os.path.basename(file_path)\n",
    "#                     accession_num, filing_date = filename.replace('.htm', '').split('_')\n",
    "                    \n",
    "#                     # Read and clean the file content\n",
    "#                     with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#                         content = f.read()\n",
    "                        \n",
    "#                     # Parse HTML and extract clean text\n",
    "#                     tree = HTMLParser(content)\n",
    "                    \n",
    "#                     # Remove script and style elements\n",
    "#                     for tag in tree.css('script'):\n",
    "#                         tag.decompose()\n",
    "#                     for tag in tree.css('style'):\n",
    "#                         tag.decompose()\n",
    "                    \n",
    "#                     # Extract clean text\n",
    "#                     if tree.body:\n",
    "#                         clean_text = tree.body.text(separator='\\n')\n",
    "#                         # Clean up extra whitespace\n",
    "#                         clean_text = ' '.join(clean_text.split())\n",
    "#                     else:\n",
    "#                         clean_text = \"\"\n",
    "                    \n",
    "#                     # Classify filing type\n",
    "#                     filing_type = self.classify_filing_type(clean_text, filename)\n",
    "                    \n",
    "#                     filing_info = {\n",
    "#                         'ticker': ticker,\n",
    "#                         'accession_number': accession_num,\n",
    "#                         'filing_date': filing_date,\n",
    "#                         'file_path': file_path,\n",
    "#                         'file_size': os.path.getsize(file_path),\n",
    "#                         'content': clean_text,\n",
    "#                         'content_length': len(clean_text),\n",
    "#                         'filing_type': filing_type  # Add filing type column\n",
    "#                     }\n",
    "#                     filings_data.append(filing_info)\n",
    "                    \n",
    "#                 except Exception as e:\n",
    "#                     self.logger.error(f\"Error processing file {file_path}: {str(e)}\")\n",
    "#                     continue\n",
    "            \n",
    "#             # Convert to DataFrame\n",
    "#             df = pd.DataFrame(filings_data)\n",
    "            \n",
    "#             # Convert dates and format columns\n",
    "#             df['filing_date'] = pd.to_datetime(df['filing_date'])\n",
    "#             df['file_size'] = df['file_size'] / 1024  # Convert to KB\n",
    "            \n",
    "#             # Sort by filing date\n",
    "#             df = df.sort_values('filing_date', ascending=False)\n",
    "            \n",
    "#             return df\n",
    "#         except Exception as e:\n",
    "#             self.logger.error(f\"Error loading filings for {ticker}: {str(e)}\")\n",
    "#             return pd.DataFrame()\n",
    "\n",
    "#     def load_company_concepts(self, ticker: str) -> pd.DataFrame:\n",
    "#         \"\"\"Load Company Concepts for a specific ticker.\"\"\"\n",
    "#         try:\n",
    "#             # Define a mapping of tickers to their CIK (Central Index Key) for easier lookup\n",
    "#             cik_mapping = {\n",
    "#                 'AAPL': '0000320193',\n",
    "#                 'MSFT': '0000789019',\n",
    "#                 'GOOGL': '0001652044',\n",
    "#                 # Add more mappings here...\n",
    "#             }\n",
    "            \n",
    "#             # Look up the CIK for the given ticker\n",
    "#             cik = cik_mapping.get(ticker, None)\n",
    "#             if not cik:\n",
    "#                 raise ValueError(f\"No CIK found for ticker {ticker}\")\n",
    "\n",
    "#             # Construct the path to the company concepts JSON file for the given ticker\n",
    "#             concepts_file_path = os.path.join(self.concepts_dir, f'CIK{cik}.json')\n",
    "            \n",
    "#             if not os.path.exists(concepts_file_path):\n",
    "#                 raise FileNotFoundError(f\"No company concepts file found for {ticker} at {concepts_file_path}\")\n",
    "            \n",
    "#             # Load and parse the JSON file\n",
    "#             with open(concepts_file_path, 'r', encoding='utf-8') as f:\n",
    "#                 concepts_data = json.load(f)\n",
    "\n",
    "#             # Extract relevant data from the JSON structure\n",
    "#             company_info = {\n",
    "#                 'cik': concepts_data.get('cik', ''),\n",
    "#                 'entity_name': concepts_data.get('entityName', ''),\n",
    "#                 'facts': concepts_data.get('facts', {})\n",
    "#             }\n",
    "\n",
    "#             # Flatten the 'facts' part of the data\n",
    "#             facts = company_info['facts']\n",
    "#             facts_flattened = {k: v for sub_dict in facts.values() for k, v in sub_dict.items()}\n",
    "\n",
    "#             # Merge the general company information with the flattened facts\n",
    "#             company_info.update(facts_flattened)\n",
    "\n",
    "#             # Convert the collected information into a DataFrame\n",
    "#             df = pd.DataFrame([company_info])\n",
    "\n",
    "#             # Add ticker as a reference column\n",
    "#             df['ticker'] = ticker\n",
    "            \n",
    "#             return df\n",
    "#         except Exception as e:\n",
    "#             self.logger.error(f\"Error loading company concepts for {ticker}: {str(e)}\")\n",
    "#             return pd.DataFrame()\n",
    "\n",
    "#     def load_all_data(self, tickers: Optional[List[str]] = None) -> Dict[str, pd.DataFrame]:\n",
    "#         \"\"\"Load filings and company concepts data for multiple tickers and return two separate DataFrames.\"\"\"\n",
    "#         all_filings_data = []  # List to hold filings data for all tickers\n",
    "#         all_concepts_data = []  # List to hold company concepts data for all tickers\n",
    "\n",
    "#         if tickers is None:\n",
    "#             # Get all tickers from the base directory\n",
    "#             tickers = [d for d in os.listdir(self.base_dir) if os.path.isdir(os.path.join(self.base_dir, d))]\n",
    "        \n",
    "#         for ticker in tickers:\n",
    "#             self.logger.info(f\"Loading filings and company concepts for {ticker}...\")\n",
    "            \n",
    "#             # Load SEC filings\n",
    "#             filings_df = self.load_filings(ticker)\n",
    "#             if not filings_df.empty:\n",
    "#                 all_filings_data.append(filings_df)\n",
    "#                 self.logger.info(f\"Loaded {len(filings_df)} filings for {ticker}\")\n",
    "            \n",
    "#             # Load Company Concepts\n",
    "#             concepts_df = self.load_company_concepts(ticker)\n",
    "#             if not concepts_df.empty:\n",
    "#                 all_concepts_data.append(concepts_df)\n",
    "#                 self.logger.info(f\"Loaded company concepts for {ticker}\")\n",
    "        \n",
    "#         # Concatenate all dataframes into a single DataFrame for each category\n",
    "#         filings_df = pd.concat(all_filings_data, ignore_index=True) if all_filings_data else pd.DataFrame()\n",
    "#         company_concepts_df = pd.concat(all_concepts_data, ignore_index=True) if all_concepts_data else pd.DataFrame()\n",
    "        \n",
    "#         return filings_df, company_concepts_df\n",
    "\n",
    "# # Example usage\n",
    "# loader = SECFilingLoader('sec_data', 'company_concepts')\n",
    "\n",
    "# # Load data for specific tickers\n",
    "# tickers = ['AAPL', 'MSFT', 'GOOGL']\n",
    "\n",
    "# # Load all filings and company concepts and get two separate DataFrames\n",
    "# filings_df, company_concepts_df = loader.load_all_data(tickers)\n",
    "\n",
    "# # Example analysis\n",
    "# print(f\"\\nTotal rows in filings_df: {len(filings_df)}\")\n",
    "# print(f\"Total rows in company_concepts_df: {len(company_concepts_df)}\")\n",
    "\n",
    "# # Optionally, preview the first few rows\n",
    "# print(\"\\nFirst few rows of filings_df:\")\n",
    "# print(filings_df[['ticker', 'accession_number', 'filing_date', 'filing_type', 'content_length']].head())\n",
    "\n",
    "# print(\"\\nFirst few rows of company_concepts_df:\")\n",
    "# print(company_concepts_df[['ticker', 'entity_name', 'cik']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
